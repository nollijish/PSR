{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52392431-8dcb-4e8c-9686-b84b5a590b05",
   "metadata": {
    "id": "52392431-8dcb-4e8c-9686-b84b5a590b05"
   },
   "source": [
    "# Background\n",
    "There are 3 Generator units at the facility that generate electricity for sale into the wholesale energy markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a821601-47af-4109-99bc-5a053959aebc",
   "metadata": {
    "id": "4a821601-47af-4109-99bc-5a053959aebc"
   },
   "source": [
    "## Motivation\n",
    "Current entry process for generation forecasting and wholesale energy sales is a static number input by Production that is not varied based on the hourly weather information. It is updated with consideration for prior day's generator performance when adjusting output expectations (single static value generally for each 24-hour period unless there is a planned outage). \n",
    "\n",
    "The desire is to:\n",
    "- Minimize excess energy generation where more energy is generated than was sold.\n",
    "- Minimize under generation where energy must be purchased on the energy imbalance market to makeup for the delta between what was sold and what was generated for the time slot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d03d2-9805-433d-b572-bc9bff430737",
   "metadata": {
    "id": "fc3d03d2-9805-433d-b572-bc9bff430737"
   },
   "source": [
    "## Questions That Are Being Addressed\n",
    "\n",
    "Can we accurately predict when the generator is operating at maximum power output capability using live data (referred to as Baseload)?\n",
    "\n",
    "Can a Machine Learning (ML) Model be used to predict a Generator's maximum power output capability for the next 7 days based on weather forecasts, current operating performance, and current fuel gas content? \n",
    "\n",
    "Is it possible to improve power output capability through controllable variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10b4ef-3da2-40a9-b9ff-0cdc3e65ff37",
   "metadata": {
    "id": "5d10b4ef-3da2-40a9-b9ff-0cdc3e65ff37"
   },
   "source": [
    "## Libraries\n",
    "\n",
    "For the final project, the goal will be to use 'standard' Python libraries that do not require specific hardware or other software packages installed.\n",
    "\n",
    "For real-life implementation, xgboost and tensorflow will be used to attempt better optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354295e-69c3-46c5-971d-f78eafa50f24",
   "metadata": {
    "id": "b354295e-69c3-46c5-971d-f78eafa50f24"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV as SearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb60af4-919e-43d8-a995-6c95ba3782e4",
   "metadata": {
    "id": "2fb60af4-919e-43d8-a995-6c95ba3782e4"
   },
   "source": [
    "# Build the Dataset and Cross-reference Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355b7af-7bc0-4e99-a62e-7cd7e4321ded",
   "metadata": {
    "id": "e355b7af-7bc0-4e99-a62e-7cd7e4321ded"
   },
   "source": [
    "## Read in the Dataset. \n",
    "\n",
    "Dataset was pulled from the facility's Process Historian using OsiSoft PI Datalink addon in Excel. The resulting values were then exported as .csv file.\n",
    "\n",
    "Rarely is the data 'clean' enough where a column will get pulled in as anything but an object/string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41885659-8490-488e-838b-91d8c3e17415",
   "metadata": {
    "id": "41885659-8490-488e-838b-91d8c3e17415"
   },
   "outputs": [],
   "source": [
    "# function to just sort out my personal dataset access shenanigans\n",
    "def dataset_read(filename):\n",
    "    url = 'https://raw.githubusercontent.com/nollijish/datasets/main/' + filename\n",
    "    return pd.read_csv(url,\n",
    "                       on_bad_lines='warn',\n",
    "                       low_memory=False,\n",
    "                       compression='gzip'\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZSCENxsIBzHB",
   "metadata": {
    "id": "ZSCENxsIBzHB"
   },
   "outputs": [],
   "source": [
    "df = dataset_read('Cogen_pwr_20221117-20060101.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e78e2e-2cf0-4862-be5c-d62a910473f5",
   "metadata": {
    "id": "a3e78e2e-2cf0-4862-be5c-d62a910473f5"
   },
   "source": [
    "## Setup Tags Lists\n",
    "\n",
    "There are 3 generator units with individual 'tags' and then common fuel gas information tags and common weather tags.\n",
    "\n",
    "Additionally, there are new tags and old tags. The old tags get shut off at some specific time and then the new tags get activated.\n",
    "\n",
    "We will discuss the information on each of the individual tags in Section 3.7 after cleaning/combining operations are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b7691-5421-4856-ad7c-d950acae42a4",
   "metadata": {
    "id": "cc8b7691-5421-4856-ad7c-d950acae42a4"
   },
   "outputs": [],
   "source": [
    "# setup for mapping tags between older tags and newer tags to create a more \n",
    "# complete dataset the formatting is for ease of interpretation\n",
    "old_tags = [ # power output old tags\n",
    "    ['1DW.PV','2DW.PV','3DW.PV']\n",
    "    , # unit 1 old tags\n",
    "    ['1WQJ.PV','1SPSJ.PV','1STSJ.PV','1CTIM.PV','1CTDA1.PV',\n",
    "     '1CPD.PV','1AFPAP.PV','1TTXC.PV','90FGCOGEN1GTG']\n",
    "    , # unit 2 old tags\n",
    "    ['2WQJ.PV','2SPSJ.PV','2STSJ.PV','2CTIM.PV','2CTDA1.PV',\n",
    "     '2CPD.PV','2AFPAP.PV','2TTXC.PV','90FGCOGEN2GTG']\n",
    "    , # unit 3 old tags\n",
    "    ['3WQJ.PV','3SPSJ.PV','3STSJ.PV','3CTIM.PV','3CTDA1.PV',\n",
    "     '3CPD.PV','3AFPAP.PV','3TTXC.PV','90FGCOGEN3GTG']\n",
    "    , # weather old tags\n",
    "    ['WTHR_PRESS_IN','WTHR_RH','WTHR_T_LOWER_F','WTHR_SPEED_MPH','WTHR_DIR_DEG']\n",
    "]\n",
    "\n",
    "new_tags = [ # power output new tags\n",
    "    ['901DW.PV','902DW.PV','903DW.PV',]\n",
    "    , # unit 1 new tags\n",
    "    ['901WQJ.PV','901SPSJ.PV','901STSJ.PV','901CTIM.PV','901CTDA1.PV',\n",
    "     '901CPD.PV','901AFPAP.PV','901TTXC.PV','90AQ137.FGCOGEN1GTG.PV']\n",
    "    , # unit 2 new tags\n",
    "    ['902WQJ.PV','902SPSJ.PV','902STSJ.PV','902CTIM.PV','902CTDA1.PV',\n",
    "     '902CPD.PV','902AFPAP.PV','902TTXC.PV','90AQ237.FGCOGEN2GTG.PV']\n",
    "    , # unit 3 new tags\n",
    "    ['903WQJ.PV','903SPSJ.PV','903STSJ.PV','903CTIM.PV','903CTDA1.PV',\n",
    "     '903CPD.PV','903AFPAP.PV','903TTXC.PV','90AQ337.FGCOGEN3GTG.PV']\n",
    "    , # weather new tags\n",
    "    ['41AI117E.PV','41AI117G.PV','41AI117A.PV','41AI117B.PV','41AI117C.PV']\n",
    "]\n",
    "\n",
    "# fuel gas information tags\n",
    "fuel_tags = ['90FR506.PV','90FR504B.PV','90AR500.PV','90AR500A.PV',\n",
    "             '90AR500R.PV','90AR500AA.PV','90AR500S.PV','90AR500C.PV',\n",
    "             '90AR500D.PV','90AR500E.PV','90AR500F.PV','90AR500G.PV',\n",
    "             '90AR500K.PV','90AR500L.PV','90AR500M.PV','90AR500N.PV',\n",
    "             '90AR500Q.PV']\n",
    "\n",
    "# common tag identifiers for the unit tags once converted to MultiIndex\n",
    "comm_tags = ['DW','WQJ','SPSJ','STSJ','CTIM','CTDA1',\n",
    "             'CPD','AFPAP','TTXC','FG']\n",
    "\n",
    "# explanatory and response variable tags\n",
    "ac = ['TIMESTAMP'] + comm_tags + fuel_tags + new_tags[4]\n",
    "exp_clf = ac[1:-1]\n",
    "resp_clf = 'BLOAD'\n",
    "exp_reg = comm_tags[1:4] + fuel_tags[2:] + new_tags[4] + ['CEFF','AHR']\n",
    "resp_reg = 'DW'\n",
    "\n",
    "tags_dict = {}\n",
    "for i in range(len(old_tags)):\n",
    "    tags_dict.update(dict(zip(new_tags[i],old_tags[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ecbd6-c77c-4d56-8b8b-bf176311cd6b",
   "metadata": {
    "id": "161ecbd6-c77c-4d56-8b8b-bf176311cd6b"
   },
   "source": [
    "# Clean and Organize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f5806-1de9-49fe-a3cc-07e55842f577",
   "metadata": {
    "id": "e13f5806-1de9-49fe-a3cc-07e55842f577"
   },
   "outputs": [],
   "source": [
    "# make a flat list out of a list of lists\n",
    "def flat(list__):\n",
    "    return [item for sublist in list__ for item in sublist]\n",
    "\n",
    "def munch_crunch_clean(df__):\n",
    "    # search for all the strings in the process historian that contains bad data\n",
    "    badpv = ['No Data','Bad Data','Bad','Intf Shut','Bad Input','I/O Timeout',\n",
    "             'Configure','Scan Off','Out of Serv','Comm Fail','','Error',\n",
    "             'Pt Created', 'Calc Failed','Invalid Float']\n",
    "    # replace those strings with null values (which we can manage easier)\n",
    "    df__ = df__.replace(to_replace=badpv,\n",
    "                        value=np.nan,\n",
    "                        inplace=False\n",
    "                        )\n",
    "    # search for empty strings using regex (not sure why above not working)\n",
    "    df__ = df__.replace(r'^\\s*$',np.nan,inplace=False,regex=True)\n",
    "    \n",
    "    # swap values over from old tags to new tags\n",
    "    for i in new_tags:\n",
    "        for k in i:\n",
    "            mask_new = df__[k].isna()\n",
    "            val = tags_dict[k]\n",
    "            df__.loc[mask_new, k] = df__.loc[mask_new, val]\n",
    "    # cut down to just the needed columns now that they have been combined\n",
    "    df__ = df__.drop(columns=flat(old_tags))\n",
    "    \n",
    "    # create unit tags list and mapping to common tags for each of the models\n",
    "    dfl = [[],[],[],[]]\n",
    "    for i in range(1,4):\n",
    "        unit_tags = ['TIMESTAMP'] + [new_tags[0][i-1]] \\\n",
    "                    + new_tags[i] + fuel_tags + new_tags[4]\n",
    "        map_dict = dict(zip(unit_tags,ac))\n",
    "        dfl[i] = df__.loc[:,unit_tags].copy()\n",
    "        dfl[i].rename(columns=map_dict,inplace=True)\n",
    "        \n",
    "    # concatenate back down to a single dataframe using multiindexing\n",
    "    df__ = pd.concat([dfl[1], dfl[2], dfl[3]],\n",
    "                     keys=['Unit1','Unit2','Unit3'],\n",
    "                     names=['UNIT','CASE']\n",
    "                     )\n",
    "    # remove rows with NaN values\n",
    "    df__ = df__.dropna(how='any',inplace=False)\n",
    "    \n",
    "    # convert the TIMESTAMP to datetime format\n",
    "    try:\n",
    "        df__['TIMESTAMP'] = pd.to_datetime(arg=df__.loc[:,'TIMESTAMP'],\n",
    "                                           errors='raise',\n",
    "                                           format=\"%m/%d/%Y %H:%M\"\n",
    "                                           )\n",
    "    except:\n",
    "        df__['TIMESTAMP'] = pd.to_datetime(arg=df__.loc[:,'TIMESTAMP'],\n",
    "                                           errors='coerce',\n",
    "                                           format=\"%d-%b-%y %H:%M:%S\"\n",
    "                                           )\n",
    "    # convert all sample values to float\n",
    "    df__.loc[:,ac[1:]] = df__.loc[:,ac[1:]].astype(dtype=np.float64)\n",
    "    \n",
    "    # remove rows with values less than standard operating values\n",
    "    mask_pwr = df__['DW'] > 30\n",
    "    df__ = df__[mask_pwr]\n",
    "    # remove rows with 0 values\n",
    "    mask_zero = (df__ != 0).all(axis=1)\n",
    "    df__ = df__[mask_zero]\n",
    "\n",
    "    # calculate heat rate and add as a column\n",
    "    x__ = df__['FG']\n",
    "    y__ = df__['90AR500.PV']\n",
    "    z__ = df__['DW']\n",
    "    df__['AHR'] = 0.9 * (x__ * y__) / z__\n",
    "\n",
    "    # calculate compressor efficiency and add as a column\n",
    "    w__ = df__['AFPAP']*0.491154\n",
    "    x__ = df__['CTIM'] + 460\n",
    "    y__ = df__['CTDA1'] + 460\n",
    "    z__ = df__['CPD'] + 14.7\n",
    "    df__['CEFF'] = 100 * x__ / (y__ - x__) * ((z__ / w__)**0.2857 - 1)\n",
    "\n",
    "    # added datetime filtering to see if more recent values make a better ML\n",
    "    mask_date = df__['TIMESTAMP'] > '2020-01-01'\n",
    "    return df__[mask_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OEYKK3Gg6RLt",
   "metadata": {
    "id": "OEYKK3Gg6RLt"
   },
   "outputs": [],
   "source": [
    "df = munch_crunch_clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c2eda-104d-4b00-9f25-5400048a2e07",
   "metadata": {
    "id": "f55c2eda-104d-4b00-9f25-5400048a2e07"
   },
   "source": [
    "## Dataframe Columns Information\n",
    "| Tag (Variable) | Variable Type | Data Type | Units | Description |\n",
    "| :- | :- | :- | :- | :- |\n",
    "| **General Data** |\n",
    "| TIMESTAMP | Categorical Ordinal | datetime | Month/Day/Year Hour:Minutes | Time at observation |\n",
    "| **Generator Data** |\n",
    "| DW | Numerical Continuous | float | Megawatts (MW) | Generator Power Output |\n",
    "| WQJ | Numerical Continuous | float | pounds/second (lb/s) | NOx Steam Injection Flow Rate |\n",
    "| SPSJ | Numerical Continuous | float | pounds/inch$^2$ (psig) | NOx Steam Pressure |\n",
    "| STSJ | Numerical Continuous | float | degrees Fahrenheit (°F) | NOx Steam Temperature |\n",
    "| CTIM | Numerical Continuous | float | degrees Fahrenheit (°F) | Turbine Compressor Inlet Temperature |\n",
    "| CTDA1 | Numerical Continuous | float | degrees Fahrenheit (°F) | Turbine Compressor Discharge Temperature |\n",
    "| CPD | Numerical Continuous | float | pounds/inch$^2$ (psig) | Turbine Compressor Outlet Pressure |\n",
    "| AFPAP | Numerical Continuous | float | inches of mercury (inHg) | Turbine Compressor Inlet Pressure |\n",
    "| TTXC | Numerical Continuous | float | degrees Fahrenheit (°F) | Turbine Exhaust Temperature Average |\n",
    "| FG | Numerical Continuous | float | thousand scuffs per hour (mscf/h) | Fuel Gas Burned |\n",
    "| CEFF | Numerical Continuous | float | percent (%) | Compressor Efficiency |\n",
    "| AHR | Numerical Continuous | float | BTU per kWh (BTU/kWh) | Actual Heat Rate |\n",
    "| **Fuel Gas Data** |\n",
    "| 90FR506.PV | Numerical Continuous | float | thousand scuffs per hour (mscf/h) | Natural Gas Total Flow All |\n",
    "| 90FR504B.PV | Numerical Continuous | float | thousand scuffs per hour (mscf/h) | Cat Gas Total Flow All |\n",
    "| 90AR500.PV | Numerical Continuous | float | BTU per scuff (BTU/scf) | Mixed Gas energy content |\n",
    "| 90AR500A.PV | Numerical Continuous | float | ratio | Cogen Mixed Gas Specific Gravity |\n",
    "| 90AR500R.PV | Numerical Continuous | float | ratio | Cogen Mixed Gas Molecular Weight |\n",
    "| 90AR500AA.PV | Numerical Continuous | float | percent (%) | Mixed Gas Hydrogen Content |\n",
    "| 90AR500S.PV | Numerical Continuous | float | percent (%) | Mixed Gas Carbon Content |\n",
    "| 90AR500C.PV | Numerical Continuous | float | percent (%) | Mixed Gas Propane Content |\n",
    "| 90AR500D.PV | Numerical Continuous | float | percent (%) | Mixed Gas Propylene Content |\n",
    "| 90AR500E.PV | Numerical Continuous | float | percent (%) | Mixed Gas Isobutane Content |\n",
    "| 90AR500F.PV | Numerical Continuous | float | percent (%) | Mixed Gas Butane Content |\n",
    "| 90AR500G.PV | Numerical Continuous | float | percent (%) | Mixed Gas Butenes Content |\n",
    "| 90AR500K.PV | Numerical Continuous | float | percent (%) | Mixed Gas Nitrogen Content |\n",
    "| 90AR500L.PV | Numerical Continuous | float | percent (%) | Mixed Gas Methane Content |\n",
    "| 90AR500M.PV | Numerical Continuous | float | percent (%) | Mixed Gas Ethane Content |\n",
    "| 90AR500N.PV | Numerical Continuous | float | percent (%) | Mixed Gas Ethylene Content |\n",
    "| 90AR500Q.PV | Numerical Continuous | float | percent (%) | Mixed Gas C5+ Content |\n",
    "| **Weather Data** |\n",
    "| 41AI117E.PV | Numerical Continuous | float | inches of mercury (inHg) | Barometric Pressure  |\n",
    "| 41AI117G.PV | Numerical Continuous | float | percent (%) | Relative Humidity |\n",
    "| 41AI117A.PV | Numerical Continuous | float | degrees Fahrenheit (°F) | Ambient Temperature |\n",
    "| 41AI117B.PV | Numerical Continuous | float | miles/h (mph) | Wind Speed |\n",
    "| 41AI117C.PV | Numerical Continuous | float | compass degree | Wind Direction |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30abf48-dde9-4c43-99b0-1a0691dcb5bf",
   "metadata": {
    "id": "b30abf48-dde9-4c43-99b0-1a0691dcb5bf"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OYaRfVIcCT_S",
   "metadata": {
    "id": "OYaRfVIcCT_S"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PAvZDaH2Hmzo",
   "metadata": {
    "id": "PAvZDaH2Hmzo"
   },
   "source": [
    "## Trends of Machine Performance and Weather Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TA41YMAs9I27",
   "metadata": {
    "id": "TA41YMAs9I27"
   },
   "outputs": [],
   "source": [
    "# compute rolling averages to smooth out the lineplots\n",
    "r = 24*14 # r is in hours\n",
    "df_trend = df.copy()\n",
    "ceff_str = 'CEFFr{}'.format(r)\n",
    "ahr_str = 'AHRr{}'.format(r)\n",
    "pwr_str = 'DWr{}'.format(r)\n",
    "temp_str = '41AI117A.PVr{}'.format(r)\n",
    "for u in ['Unit1','Unit2','Unit3']:\n",
    "    df_trend.loc[u,ceff_str] = df_trend.loc[u,'CEFF'].rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,ahr_str] = df_trend.loc[u,'AHR'].rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,temp_str] = df_trend.loc[u,'41AI117A.PV'].rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,pwr_str] = df_trend.loc[u,'DW'].rolling(r,min_periods=1).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EcV2FRFRE5qC",
   "metadata": {
    "id": "EcV2FRFRE5qC"
   },
   "outputs": [],
   "source": [
    "# I made this graph near the end. Most of my lines of code are spent on\n",
    "# formatting figures.\n",
    "sns.set_style('whitegrid')\n",
    "perf_trend, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
    "ax2 = axes[0].twinx()\n",
    "ax3 = axes[1].twinx()\n",
    "ax4 = axes[2].twinx()\n",
    "\n",
    "sns.lineplot(data=df_trend.reset_index(),\n",
    "             x='TIMESTAMP',\n",
    "             y=ahr_str,\n",
    "             hue='UNIT',\n",
    "             alpha = 0.5,\n",
    "             ax=axes[0]\n",
    "             )\n",
    "axes[0].set_title('Trend of Heat Rate Over Time')\n",
    "axes[0].set_ylabel('Actual Heat Rate (BTU/kWh)')\n",
    "axes[0].set_ylim(9500, 11600)\n",
    "ax2.plot(df_trend.loc[u,'TIMESTAMP'],\n",
    "         df_trend.loc[u,temp_str],\n",
    "         'r-',\n",
    "         alpha=0.5\n",
    "         )\n",
    "ax2.set_ylabel('Weather Temperature (°F)',color='r')\n",
    "ax2.set_ylim(df_trend[temp_str].min()-2, df_trend[temp_str].max()+2)\n",
    "\n",
    "sns.lineplot(data=df_trend.reset_index(),\n",
    "             x='TIMESTAMP',\n",
    "             y=ceff_str,\n",
    "             hue='UNIT',\n",
    "             alpha = 0.5,\n",
    "             ax=axes[1]\n",
    "             )\n",
    "axes[1].set_ylim(87, 91)\n",
    "axes[1].set_title('Trend of Compressor Efficiency Over Time')\n",
    "axes[1].set_ylabel('Compressor Efficiency (%)')\n",
    "ax3.plot(df_trend.loc[u,'TIMESTAMP'],\n",
    "         df_trend.loc[u,temp_str],\n",
    "         'r-',\n",
    "         alpha=0.5\n",
    "         )\n",
    "ax3.set_ylabel('Weather Temperature (°F)',color='r')\n",
    "ax3.set_ylim(df_trend[temp_str].min()-2, df_trend[temp_str].max()+2)\n",
    "\n",
    "sns.lineplot(data=df_trend.reset_index(),\n",
    "             x='TIMESTAMP',\n",
    "             y=pwr_str,\n",
    "             hue='UNIT',\n",
    "             alpha = 0.5,\n",
    "             ax=axes[2]\n",
    "             )\n",
    "axes[2].set_ylim(32, 45)\n",
    "axes[2].set_title('Trend of Generator Power Output Over Time')\n",
    "axes[2].set_ylabel('Power Output (MW)')\n",
    "ax4.plot(df_trend.loc[u,'TIMESTAMP'],\n",
    "         df_trend.loc[u,temp_str],\n",
    "         'r-',\n",
    "         alpha=0.5\n",
    "         )\n",
    "ax4.set_ylabel('Weather Temperature (°F)',color='r')\n",
    "ax4.set_ylim(df_trend[temp_str].min()-2, df_trend[temp_str].max()+2)\n",
    "\n",
    "perf_trend.suptitle('Machine Trends vs Weather Temperature')\n",
    "perf_trend.subplots_adjust(top=0.90,hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057d080-95d8-445b-b020-88c9ef152047",
   "metadata": {
    "id": "a057d080-95d8-445b-b020-88c9ef152047"
   },
   "source": [
    "# Baseload analysis\n",
    "\n",
    "Baseload operation is when the generator is being operated at full power output capability. The Baseload flag is not being pulled over into the Process Historian so we will need to develop a mechanism to determine when the units are operating as Baseload. These generators only have three operating modes:\n",
    "1. Shutdown\n",
    "2. Setpoint\n",
    "3. Baseload\n",
    "\n",
    "This is important as we only want to train the power output capability regressors on the data where the units are operating as Baseloaded. The Shutdown conditions have already been filtered out, leaving just Shutdown and Baseload operation in the dataset. With that in mind, we can simplify it to a binary system where Baseload is either True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de88e1-9713-41a0-a261-66987327787b",
   "metadata": {
    "id": "b6de88e1-9713-41a0-a261-66987327787b"
   },
   "source": [
    "## Manually Assign Baseloading to Train ML\n",
    "Using some domain knowledge we are aware of trends in the operation of a generator on a daily basis when it is baseloaded. Essentially, if the power output varies sinussoidally and with weather temperature, we can assess that it is Baseloaded. If the power output is almost dead steady, it is not Baseloaded. In this section we will analyze a random sample of daily values, create a computational estimate if it is Baseloaded, compare that estimate to a manual test and then assess if changes need to be made to the computational estimator values.\n",
    "\n",
    "The estimator values are:\n",
    "1. Daily power output variance\n",
    "2. Daily correlation between power output and weather temperature\n",
    "3. Percent change between hourly values\n",
    "\n",
    "The percent change was added as a way to filter out values where a operation mode was changed mid-day to/from Baseload. A significant step change would create a high variance, falsely calling a day Baseloaded by the first assessment. Additionally, if the change happened and coincided with specific weather temperature changes, that would falsely flag a high correlation on the second value as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71e275-56c9-4959-9ca8-3b36e6a62696",
   "metadata": {
    "id": "0f71e275-56c9-4959-9ca8-3b36e6a62696"
   },
   "outputs": [],
   "source": [
    "# create new Baseload boolean column\n",
    "df['BLOAD'] = False\n",
    "ac.append('BLOAD')\n",
    "\n",
    "# create groupby object for daily groups\n",
    "df_gb = df.groupby(pd.Grouper(key='TIMESTAMP',freq='D',sort=True,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6842259-53d5-4981-9366-a5c7c384b73b",
   "metadata": {
    "id": "e6842259-53d5-4981-9366-a5c7c384b73b"
   },
   "outputs": [],
   "source": [
    "# graph generation where we will use our judgement to determine if the unit is \n",
    "# Baseloaded or running at setpoint\n",
    "def bload_plt(time, pwr, temp, corr, var, pcnt_d, bload, unit, name, save=False):\n",
    "    %matplotlib inline\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax1.plot(time, pwr, 'g-')\n",
    "    ax1.set_title(unit + ' ' + name)\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Power (MW)', color='g')\n",
    "    ax1.set_ylim(pwr.min()-0.2, pwr.max()+0.2)\n",
    "    ax1.text(x=time.min(),\n",
    "             y=pwr.max()+0.01,\n",
    "             s='bload: {0} \\n corr: {1:0.2f} \\n var: {2:0.2f} \\n pcnt_d: {3:0.2}'.format(bload, corr, var, pcnt_d),\n",
    "             fontsize='small',\n",
    "             verticalalignment='top'\n",
    "            )\n",
    "    ax2.plot(time, temp, 'b-')\n",
    "    ax2.set_ylabel('Weather Temperature (°F)', color='b')\n",
    "    ax2.set_ylim(temp.min(), temp.max())\n",
    "\n",
    "    path = os.path.join(os.getcwd(),'figs')\n",
    "    filename = unit + str(name)[:10] + '.png'\n",
    "    if save:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        plt.savefig(fname=os.path.join(path,filename))\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1f3c4-fd2c-4ae1-8a85-0cd4065fee68",
   "metadata": {
    "id": "e5c1f3c4-fd2c-4ae1-8a85-0cd4065fee68"
   },
   "outputs": [],
   "source": [
    "# generate Baseload sample to train a binary classifier\n",
    "# num is the number of desired days of samples to process, a lot will be filtered out\n",
    "# set tuning=True to output graphs to folder to manually adjust var,corr,pcnt_d\n",
    "def bload_samples(df_gb_date, num=500, tuning=False):\n",
    "    # create a list of the groupby keys, choose a random sample\n",
    "    df_rk_list = random.sample(population=[*df_gb_date.groups.keys()],k=num)\n",
    "\n",
    "    # use the random sample to construct a dataframe to do manual baseload assesssment on\n",
    "\n",
    "    df_sub_list = []\n",
    "    for n in df_rk_list:\n",
    "        try:\n",
    "            g = df_gb_date.get_group(n).copy()\n",
    "        except KeyError:\n",
    "            out_str = str(n) + ' is not a valid key for get_group'\n",
    "            print(out_str)\n",
    "        else:\n",
    "            if ( (g.size>0) & (g['41AI117A.PV'].var()>6) ):\n",
    "                for u in ['Unit1','Unit2','Unit3']: # iterate through the generator units separately\n",
    "                    try:\n",
    "                        var = g.loc[(u),'DW'].var()\n",
    "                        corr = g.loc[(u),['DW','41AI117A.PV']].corr().iloc[0,1]\n",
    "                        pcnt_d = g.loc[(u),'DW'].pct_change(periods=1).abs().max()\n",
    "                    except KeyError:\n",
    "                        out_str = str(u) + ' does not exist in ' + str(n)\n",
    "                        print(out_str)\n",
    "                    else:\n",
    "                        bload = False # initiate the boolean variable\n",
    "                        if ( (var>=0.13) & (corr<-0.6) & (pcnt_d<0.042) ): # notional values to assign Baseload\n",
    "                            bload = True\n",
    "                            g.loc[(u),'BLOAD'] = bload\n",
    "                        # exploratory graphs to tune the notional values\n",
    "                        bload_fig = bload_plt(time=g.loc[(u),'TIMESTAMP'],\n",
    "                                              pwr=g.loc[(u),'DW'],\n",
    "                                              temp=g.loc[(u),'41AI117A.PV'],\n",
    "                                              corr=corr,\n",
    "                                              var=var,\n",
    "                                              pcnt_d=pcnt_d,\n",
    "                                              bload=bload, \n",
    "                                              unit=u,\n",
    "                                              name=str(n),\n",
    "                                              save=tuning\n",
    "                                              )\n",
    "                        if bload:\n",
    "                            bload_true = bload_fig\n",
    "                        else:\n",
    "                            bload_false = bload_fig\n",
    "                df_sub_list.append(g)\n",
    "    return pd.concat(df_sub_list), bload_true, bload_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E59BEwEsuj7Z",
   "metadata": {
    "id": "E59BEwEsuj7Z"
   },
   "outputs": [],
   "source": [
    "df_bl, bload_true_daily, bload_false_daily = bload_samples(df_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e0609f-e59a-4154-a4c9-2e2209db2924",
   "metadata": {
    "id": "98e0609f-e59a-4154-a4c9-2e2209db2924"
   },
   "source": [
    "## Baseload Figure\n",
    "Here we can see an example of what the generator looks like running baseloaded with relation to weather temperature and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48e1fe-26d9-41c8-9136-17aa39a855b4",
   "metadata": {
    "id": "cf48e1fe-26d9-41c8-9136-17aa39a855b4"
   },
   "outputs": [],
   "source": [
    "bload_true_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfef347-89ce-40cd-86cb-654d238e4fe9",
   "metadata": {
    "id": "cbfef347-89ce-40cd-86cb-654d238e4fe9"
   },
   "source": [
    "## Not Baseloaded Figure\n",
    "Here we can see an example of what the generator looks like running to setpoint (at least partially) with relation to weather temperature and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b461d-d873-4260-8eaa-f291920a2918",
   "metadata": {
    "id": "7a4b461d-d873-4260-8eaa-f291920a2918"
   },
   "outputs": [],
   "source": [
    "bload_false_daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74236e-7b04-4c22-a627-599550169a30",
   "metadata": {
    "id": "8c74236e-7b04-4c22-a627-599550169a30"
   },
   "source": [
    "## Baseload and Exhaust Temperature\n",
    "It is speculated that the exhaust temperature is the most important feature for determining baseload operation. We will explore this further in Section 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1679aef-8a27-41db-bc36-25a2efc6ba59",
   "metadata": {
    "id": "f1679aef-8a27-41db-bc36-25a2efc6ba59"
   },
   "outputs": [],
   "source": [
    "# setup the figure and axes for exahust temperature analysis\n",
    "sns.set_style('whitegrid')\n",
    "bload_exh_temp, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "sns.histplot(data=df_bl[(df_bl['TTXC']>850)],\n",
    "             x='TTXC', \n",
    "             hue='BLOAD',\n",
    "             bins=50,\n",
    "             stat='density',\n",
    "             ax=axes[0]\n",
    "            )\n",
    "axes[0].set_title('Histogram of Exhaust Temperature')\n",
    "axes[0].set_xlabel('Exhaust Temperature (°F)')\n",
    "sns.scatterplot(data=df_bl.sample(n=1000),\n",
    "                x='41AI117A.PV',\n",
    "                y='DW',\n",
    "                hue='BLOAD',\n",
    "                alpha = 0.5,\n",
    "                ax=axes[1]\n",
    "               )\n",
    "axes[1].set_title('Scatterplot of Power Output and Weather Temperature (°F)')\n",
    "axes[1].set_xlabel('Weather Temperature (°F)')\n",
    "axes[1].set_ylabel('Power Output (MW)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba07c34-17a2-4995-977c-39e51ae6bcc7",
   "metadata": {
    "id": "bba07c34-17a2-4995-977c-39e51ae6bcc7"
   },
   "source": [
    "## Train the Baseload ML\n",
    "You may be wondering, \"why are we using an ML and not just continue using the daily groupby method to determine Baseloading.\"\n",
    "\n",
    "Good Question! We are using an ML for the following reasons:\n",
    "- I would like to use current sample values only when assessing Baseload and not look at complete daily data\n",
    "- I am hoping for a binary classifier that is very stringent on assigning Baseload = True is a better generalizer than the simple daily value prediction method\n",
    "\n",
    "Now, what do we mean when we say \"very stringent?\"\n",
    "\n",
    "Assigning the null hypothesis as the generator is not Baseloaded, a Type I error would be where we incorrectly assign the generator as operating Baseloaded when it is not operating Baseloaded. In our model, we want to heavily minimize Type I error in favor of a higher Type II error. Type II error in this case would be where we assign the generator as not operating Baseloaded when it is operating Baseloaded.\n",
    "\n",
    "H0: Generator Baseload=False\n",
    "\n",
    "HA: Generator Baseload=True\n",
    "\n",
    "| Truth | Assign Baseload=False | Assign Baseload=True |\n",
    "| :- | :- | :- |\n",
    "| Baseload=False | okay | Type I error |\n",
    "| Baseload=True| Type II error | okay |\n",
    "\n",
    "Binary Classifiers that allow the use of p-values and ROC/precision scoring are very handy for tuning this decision boundary outside of the model's hyperparameters.\n",
    "\n",
    "The Type I error rate may matter if we over-filter to the point where there are not enough valid observations to effectively train the Regressor model. We will evaluate this later to determine that our dataset is still effective with relation to its dimensionality when we get to the Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedc3b7-efa4-40f1-a526-a258767d2941",
   "metadata": {
    "id": "4aedc3b7-efa4-40f1-a526-a258767d2941"
   },
   "outputs": [],
   "source": [
    "# setup our feature space, target, and our train/test datasets\n",
    "X = df_bl.loc[:,exp_clf]\n",
    "y = df_bl.loc[:,resp_clf]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148eb303-544f-473b-a824-77f76e148a3e",
   "metadata": {
    "id": "148eb303-544f-473b-a824-77f76e148a3e"
   },
   "source": [
    "**Pipeline bonanza!**\n",
    "\n",
    "Testing pipelines utilizing scaling and a classifier with a Cross Validation search. Randomized search is used as it is more effective than a straight search for finding optimal model hyperparameters. Bayesian methods are not used at this time for hyperparameter searching. Dimension reduction was removed from the pipeline as: the dataset is already a relatively low dimension, it tended to increase computation time for no noticed benefit, and it caused some issues with the SVM that were not easily resolvable.\n",
    "\n",
    "The model we will be using is the Gradient Tree Boosting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d32815-f31c-434a-8591-04d6196aeee7",
   "metadata": {
    "id": "e3d32815-f31c-434a-8591-04d6196aeee7"
   },
   "outputs": [],
   "source": [
    "# Gradient Tree Boosting\n",
    "def pipeGBT(X__,y__,ml__='clf',n_iter__=6):\n",
    "  \n",
    "    # setup conditional variables for either classifier or regressor\n",
    "    if ml__=='reg':\n",
    "        scoring__ = 'r2'\n",
    "        model__ = GradientBoostingRegressor()\n",
    "    else:\n",
    "        scoring__ = 'roc_auc'\n",
    "        model__ = GradientBoostingClassifier()\n",
    "\n",
    "    # setup pipeline steps\n",
    "    pipe__ = Pipeline([('sclr', 'passthrough'),\n",
    "                       ('ML', 'passthrough')\n",
    "                       ]\n",
    "                      )\n",
    "    # setup gridsearchCV parameter grid\n",
    "    param_grid__ = [{'sclr': [StandardScaler(), MinMaxScaler(), 'passthrough'],\n",
    "                     'ML': [model__],\n",
    "                     'ML__learning_rate': stats.loguniform(1e-3,3e-1),\n",
    "                     'ML__n_estimators': np.ceil(stats.loguniform.rvs(2e1,5e2,size=2*n_iter__)).astype(int).tolist(),\n",
    "                     'ML__criterion': ['friedman_mse', 'squared_error'],\n",
    "                     'ML__max_depth': stats.randint(3,20),\n",
    "                     'ML__subsample': stats.uniform(0.01, 0.99)\n",
    "                     }\n",
    "                    ]\n",
    "    return SearchCV(pipe__,\n",
    "                    param_grid__,\n",
    "                    n_iter=n_iter__,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scoring__,\n",
    "                    verbose=3\n",
    "                    ).fit(X__,y__).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JFjUiAio8ctL",
   "metadata": {
    "id": "JFjUiAio8ctL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = pipeGBT(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f56503-55f6-45a3-a8b8-27b6861fa2a3",
   "metadata": {
    "id": "97f56503-55f6-45a3-a8b8-27b6861fa2a3"
   },
   "source": [
    "## Analyze the Baseload ML Model\n",
    "The wonderful part of binary classifiers is how much opportunity there is in tuning and analysis. It is definitely the funnest type of ML to explore and tune.\n",
    "\n",
    "Per our discussion in Section 4.3, our aim is to minimize Type I error. This can be directly labeled as the False Positive Rate of the ensuing classifier performance.\n",
    "\n",
    "With that in mind, here we will analyze the False Positive Rate and the other aspects of the model to see where the conceptual \"best\" location is to set the p-value decision boundary. In section 4.5 we will analyze the impact of the decision boundary on available samples for training the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf7e40-2ab8-4b34-8f8f-500e08745474",
   "metadata": {
    "id": "d3cf7e40-2ab8-4b34-8f8f-500e08745474"
   },
   "outputs": [],
   "source": [
    "# take a peek at the winner\n",
    "print(clf.score(X_test,y_test))\n",
    "print(clf.get_params()['steps'])\n",
    "print(clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80be67d-04ac-4af6-bbe5-10b45522b9c6",
   "metadata": {
    "id": "f80be67d-04ac-4af6-bbe5-10b45522b9c6"
   },
   "outputs": [],
   "source": [
    "# Review the Receiver Operating Characteristic on the test sample\n",
    "y_score = clf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresh = metrics.roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "bload_clf_roc = metrics.RocCurveDisplay(fpr=fpr, \n",
    "                                        tpr=tpr,\n",
    "                                        roc_auc = metrics.roc_auc_score(y_test, y_score)\n",
    "                                        ).plot()\n",
    "bload_clf_roc.ax_.set_title('Receiver Operating Characteristic Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88689f02-2651-4b70-8838-e56322b374f2",
   "metadata": {
    "id": "88689f02-2651-4b70-8838-e56322b374f2"
   },
   "outputs": [],
   "source": [
    "# Figures for threshold analysis and its impact on the FPR\n",
    "bload_prop_thresh, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(thresh, fpr, 'g-')\n",
    "ax1.set_title('Impact of Threshold on FPR')\n",
    "ax1.set_xlabel('Threshold for p-value')\n",
    "ax1.set_ylabel('False Positive Rate (FPR)', color='g')\n",
    "ax1.set_xlim(0.8, 1)\n",
    "ax1.set_ylim(0, 0.01)\n",
    "\n",
    "por = np.array([(y_score >= i).sum()/y_score.size for i in thresh])\n",
    "ax2.plot(thresh, por, 'r-')\n",
    "ax2.set_ylabel('Baselod Portion of Test Sample)', color='r')\n",
    "ax2.set_ylim(0.4, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9488dc-1b6e-458b-b414-1d2b23d297d7",
   "metadata": {
    "id": "7c9488dc-1b6e-458b-b414-1d2b23d297d7"
   },
   "outputs": [],
   "source": [
    "# Take a look at feature importances for the winning classifier\n",
    "# specify number of features to see\n",
    "n = 5\n",
    "\n",
    "df_f = pd.DataFrame(data=clf._final_estimator.feature_importances_.reshape((1,len(exp_clf))),\n",
    "                    columns=exp_clf\n",
    "                   )\n",
    "k = 0\n",
    "out = 'Top ' + str(n) + ' important variables are (in order of importance):'\n",
    "while k < n:\n",
    "    out+='\\n' + str(k+1) +') {}'\n",
    "    k+=1\n",
    "print(out.format(*df_f.sort_values(by=0,axis=1,ascending=False).iloc[0,0:n].index.to_list()))\n",
    "\n",
    "bload_feat, ax1 = plt.subplots(figsize=(8, 8))\n",
    "sns.barplot(data=df_f.sort_values(by=0,axis=1,ascending=False).iloc[:,:n],\n",
    "            orient='h',\n",
    "            ax=ax1\n",
    "            )\n",
    "ax1.set(title='Feature Importance',\n",
    "        ylabel='Feature',\n",
    "        xlabel='Importance'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a599278-9911-4648-99b5-f20f746c50fc",
   "metadata": {
    "id": "8a599278-9911-4648-99b5-f20f746c50fc"
   },
   "source": [
    "## Predict Baseload\n",
    "\n",
    "Here we can see the proportion of the total dataset that will be assigned to Baseload at different threshold p-values. We can see that we are really not losing too many observations by cranking up the threshold. \n",
    "\n",
    "Default threshold is 0.5. Here we can see the impact of different threshold values on the proportion of samples that will be available to train the regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34460184-1376-463a-beb4-e59a6cf0494f",
   "metadata": {
    "id": "34460184-1376-463a-beb4-e59a6cf0494f"
   },
   "outputs": [],
   "source": [
    "# use the classifier pipeline to predict the p-values for the entire dataset\n",
    "X = df.loc[:,exp_clf]\n",
    "y = clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a4416-a493-4ee5-a496-8fab3180aacd",
   "metadata": {
    "id": "799a4416-a493-4ee5-a496-8fab3180aacd"
   },
   "outputs": [],
   "source": [
    "# proportion of values assigned BLOAD == True for different threshold values\n",
    "n = 10\n",
    "thresh = np.arange(start=0.5,stop=1.0,step=0.0005)\n",
    "por = np.array([(y >= i).sum()/y.size for i in thresh])\n",
    "arr = np.array([thresh,por]).T\n",
    "\n",
    "out = 'Dataset size: {}\\n'.format(y.size) + ''.join(\n",
    "    ['threshold: {:0.4f}, proportion: {:0.4f}\\n'.format(i[0],i[1]) \\\n",
    "     for i in arr[-n:,:]\n",
    "    ]\n",
    ")\n",
    "print(out)\n",
    "\n",
    "ax = sns.lineplot(x=thresh,y=por)\n",
    "ax.set(title='Impact of Threshold on Baseload Portion',\n",
    "       xlabel='Threshold for p-value',\n",
    "       ylabel='Baseload Portion of Full Dataset'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454ad3a-a93c-40d2-97d3-c3cb2f68189b",
   "metadata": {
    "id": "9454ad3a-a93c-40d2-97d3-c3cb2f68189b"
   },
   "source": [
    "## Filter To the Baseload Values\n",
    "By increasing the threshold to the highest value while maintaining sample proportion to at least 0.40, we lose a perfectly manageable portion of the samples while assuring our false positive rate is as low as we can reasonably make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb49eb1-45a8-4377-b2f0-eacf2dcc10e0",
   "metadata": {
    "id": "abb49eb1-45a8-4377-b2f0-eacf2dcc10e0"
   },
   "outputs": [],
   "source": [
    "# filter down to only those values where proportion remains above 0.4\n",
    "arr = arr[arr[:,1] > 0.40]\n",
    "\n",
    "# identify the maximum proportion value in this array\n",
    "decision = arr[arr[:,0].argmax(),0]\n",
    "print('Decision boundary selected: {:0.4f}'.format(decision))\n",
    "\n",
    "# filter to only those indexes where p-value > decision boundary\n",
    "mask_bload = y > decision\n",
    "df = df.loc[mask_bload]\n",
    "print('Samples remaining for regressor training: {}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd154d2a-1055-41de-a26e-b0bc1ccf0c01",
   "metadata": {
    "id": "dd154d2a-1055-41de-a26e-b0bc1ccf0c01"
   },
   "source": [
    "# Onward and upward! \n",
    "Here we will train our regressor models to predict the power output capability of the generator. The baseload classifier in Section 4 we could be a bit loosey-goosey with parameters and the pipeline because we were utilizing a small random sample from the total dataset. In this section, we need to be concise about how we train the data as there are ~50,000 observations and we don't want this to run for weeks on end.\n",
    "\n",
    "The model we will be using is the Gradient Tree Boosting Regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5cd59-eeb9-48c9-9b9a-47c3df2226ee",
   "metadata": {
    "id": "ffd5cd59-eeb9-48c9-9b9a-47c3df2226ee"
   },
   "source": [
    "## Generalized all generators model\n",
    "Here we will train the regressor for all of the generators together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f661ea17-d8b0-4118-b252-1f717e78f873",
   "metadata": {
    "id": "f661ea17-d8b0-4118-b252-1f717e78f873"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# setup our feature space, target, and our train/test datasets\n",
    "X = df.loc[:,exp_reg]\n",
    "y = df.loc[:,resp_reg]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "\n",
    "# train regressor and store the estimator in a dictionary\n",
    "reg = {}\n",
    "reg['all'] = pipeGBT(X_train,y_train,'reg')\n",
    "\n",
    "# take a peek at the winner with performance on the test data\n",
    "y_pred = {}\n",
    "y_pred['all'] = reg['all'].predict(X_test)\n",
    "print(reg['all'].score(X_test,y_test))\n",
    "print(reg['all'].get_params()['steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d061d5-8a4d-4db4-8526-d3ff278187b4",
   "metadata": {
    "id": "10d061d5-8a4d-4db4-8526-d3ff278187b4"
   },
   "source": [
    "## Unit Specific Generator Models\n",
    "Here we will train the regressor (3) times, once for each generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074acd7-2443-4939-a7ca-7459f5163c57",
   "metadata": {
    "id": "e074acd7-2443-4939-a7ca-7459f5163c57"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train unit specific regressors and store the estimators in the dictionary\n",
    "for u in ['Unit1','Unit2','Unit3']:\n",
    "    reg[u] = pipeGBT(X_train.loc[u,:],y_train.loc[u,:],'reg')\n",
    "\n",
    "# take a peek at all of the winners with performance on the test data\n",
    "for u in ['Unit1','Unit2','Unit3']:\n",
    "    y_pred[u] = reg[u].predict(X_test.loc[u,:])\n",
    "    print(u)\n",
    "    print(reg[u].score(X_test.loc[u,:],y_test.loc[u,:]))\n",
    "    print(reg[u].get_params()['steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafee4ee-1f69-4cf6-981b-6041289d52b3",
   "metadata": {
    "id": "fafee4ee-1f69-4cf6-981b-6041289d52b3"
   },
   "source": [
    "## Analyze Regressor Performance\n",
    "We can see from the plots below that even though the R$^2$ performance of the generalized model is comparable to that of the individual models, its error in prediction compared to actual values is a bit higher. This tells us that we are likely missing some performance variables that could improve the generalized model.\n",
    "\n",
    "Additionally, we can see some additional error taking off at the corner cases where the generalized model stops performing well but is managed well within the individual models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VbCPrz93UYy0",
   "metadata": {
    "id": "VbCPrz93UYy0"
   },
   "outputs": [],
   "source": [
    "j = ['all','Unit1','Unit2','Unit3']\n",
    "\n",
    "pwr_analysis, ax = plt.subplots(4, 2, figsize=(12, 20))\n",
    "for i in range(4):\n",
    "    u = j[i]\n",
    "    if u == 'all':\n",
    "        y_act__ = y_test.to_numpy()\n",
    "    else:\n",
    "        y_act__ = y_test.loc[u,:].to_numpy()\n",
    "    y_pred__ = y_pred[u]\n",
    "    y_delta__ = y_act__ - y_pred__\n",
    "    \n",
    "    sns.regplot(x=y_act__,\n",
    "                y=y_pred__,\n",
    "                line_kws={\"color\": \"C1\"},\n",
    "                ax=ax[i,0]\n",
    "                )\n",
    "    ax[i,0].set(\n",
    "        xlim=(32,46),\n",
    "        ylim=(32,46),\n",
    "        ylabel='Predicted Power (MW)',\n",
    "        xlabel='Actual Power (MW)',\n",
    "        title=u + ' Predicted vs Actual'\n",
    "    )\n",
    "    sns.regplot(x=y_act__,\n",
    "                y=y_delta__,\n",
    "                line_kws={\"color\": \"C1\"},\n",
    "                ax=ax[i,1]\n",
    "                )\n",
    "    ax[i,1].set(\n",
    "        xlim=(32,46),\n",
    "        ylim=(-1.5,1.5),\n",
    "        xlabel='Actual Power (MW)',\n",
    "        ylabel='Delta (MW)',\n",
    "        title=u + ' Delta from Actual'\n",
    "    )\n",
    "pwr_analysis.suptitle('Predicted vs Actual Power Output')\n",
    "pwr_analysis.tight_layout()\n",
    "pwr_analysis.subplots_adjust(top=0.95,hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c7cfd-b11f-4bf9-895e-d7e98bc46b72",
   "metadata": {
    "id": "ec1c7cfd-b11f-4bf9-895e-d7e98bc46b72"
   },
   "source": [
    "# Power Capability Forecasting\n",
    "\n",
    "Here we introduce a week's worth of data that the models have not seen to view how well they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eDZh_UFRMMGJ",
   "metadata": {
    "id": "eDZh_UFRMMGJ"
   },
   "outputs": [],
   "source": [
    "ac.remove('BLOAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bs-I6KLMcyJu",
   "metadata": {
    "id": "Bs-I6KLMcyJu"
   },
   "outputs": [],
   "source": [
    "# pull data from a week that the dataset has not seen before and review \n",
    "# predictions\n",
    "new_df = munch_crunch_clean(dataset_read('Cogen_pwr_20221128-20221121.csv.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CUeOYPYI1gM_",
   "metadata": {
    "id": "CUeOYPYI1gM_"
   },
   "outputs": [],
   "source": [
    "def plot_pwr_forecast(time, pwr1, pwr2, pwr3, temp, unit):\n",
    "    fig, ax1 = plt.subplots(figsize=(20, 8))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax3 = ax1.twinx()\n",
    "    ax4 = ax1.twinx()\n",
    "\n",
    "    pwr_ylim_lo = pwr1.min()-0.3\n",
    "    pwr_ylim_hi = pwr1.max()+0.3\n",
    "\n",
    "    ax1.plot(time, temp, 'b-')\n",
    "    ax1.set_title(unit + ' Power Forecast')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Weather Temperature (°F)', color='b')\n",
    "    ax1.set_ylim(temp.min(), temp.max())\n",
    "\n",
    "    ax2.plot(time, pwr1, 'g-',label='Measured')\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Power (MW)')\n",
    "    ax2.set_ylim(pwr_ylim_lo, pwr_ylim_hi)\n",
    "    ax3.plot(time, pwr2,'r-',label='Predicted Unit Model')\n",
    "    ax3.set_xlabel('Time')\n",
    "    ax3.set_ylim(pwr_ylim_lo, pwr_ylim_hi)\n",
    "    ax4.plot(time, pwr3,'m-',label='Predicted General Model')\n",
    "    ax4.set_xlabel('Time')\n",
    "    ax4.set_ylim(pwr_ylim_lo, pwr_ylim_hi)\n",
    "\n",
    "    fig.legend(title='Power (MW):')\n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0W06xmMfptf_",
   "metadata": {
    "id": "0W06xmMfptf_"
   },
   "outputs": [],
   "source": [
    "# create our power forecast using the unit specific regressors and the general regressor\n",
    "pwr_forecast = {}\n",
    "r = 8 # r is in hours\n",
    "df_trend = new_df.copy()\n",
    "\n",
    "for u in ['Unit1','Unit2','Unit3']:\n",
    "    # use rolling averages to smooth out the lineplots\n",
    "    df_trend.loc[u,'DW_pred_unit'] = pd.Series(reg[u].predict(new_df.loc[u,exp_reg])).rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,'DW_pred_all'] = pd.Series(reg['all'].predict(new_df.loc[u,exp_reg])).rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,'DW'] = new_df.loc[u,'DW'].rolling(r,min_periods=1).mean().values\n",
    "    df_trend.loc[u,'41AI117A.PV'] = new_df.loc[u,'41AI117A.PV'].rolling(r,min_periods=1).mean().values\n",
    "    pwr_forecast[u] = plot_pwr_forecast(time=df_trend.loc[u,'TIMESTAMP'], \n",
    "                                        pwr1=df_trend.loc[u,'DW'],\n",
    "                                        pwr2=df_trend.loc[u,'DW_pred_unit'],\n",
    "                                        pwr3=df_trend.loc[u,'DW_pred_all'],\n",
    "                                        temp=df_trend.loc[u,'41AI117A.PV'], \n",
    "                                        unit=u\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G0oLnwSF8-1x",
   "metadata": {
    "id": "G0oLnwSF8-1x"
   },
   "outputs": [],
   "source": [
    "pwr_forecast['Unit1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ULuXRfk79XkK",
   "metadata": {
    "id": "ULuXRfk79XkK"
   },
   "outputs": [],
   "source": [
    "pwr_forecast['Unit2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fUqy6tYU9lYo",
   "metadata": {
    "id": "fUqy6tYU9lYo"
   },
   "outputs": [],
   "source": [
    "pwr_forecast['Unit3']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:ML_cpu]",
   "language": "python",
   "name": "conda-env-ML_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
